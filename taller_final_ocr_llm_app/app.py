import os
import io
import time
import numpy as np
import streamlit as st
from PIL import Image
from dotenv import load_dotenv

# ====== Load environment variables ======
load_dotenv()
GROQ_API_KEY = os.getenv("GROQ_API_KEY", "")
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY", "")

# Lazy imports for heavy libs to speed up cold start
@st.cache_resource
def get_easyocr_reader():
    import easyocr
    # Spanish + English by default; adjust as needed
    return easyocr.Reader(['es', 'en'], gpu=False)

def do_ocr(image: Image.Image):
    reader = get_easyocr_reader()
    # easyocr expects ndarray (RGB)
    arr = np.array(image.convert("RGB"))
    results = reader.readtext(arr, detail=0)  # list of strings
    return "\n".join(results).strip()

def groq_chat_completion(prompt, model_name, temperature, max_tokens):
    try:
        from groq import Groq
        client = Groq(api_key=GROQ_API_KEY)
        resp = client.chat.completions.create(
            messages=[
                {"role": "system", "content": "You are a helpful assistant that follows the user's instructions precisely."},
                {"role": "user", "content": prompt},
            ],
            model=model_name,
            temperature=float(temperature),
            max_tokens=int(max_tokens),
        )
        return resp.choices[0].message.content
    except Exception as e:
        return f"‚ö†Ô∏è Error calling GROQ API: {e}"

# ===== Hugging Face helpers (robustos por tarea) =====
def hf_post(task: str, inputs: str, model_id: str, extra: dict | None = None):
    """Wrapper estable usando client.post para cualquier task soportada."""
    try:
        from huggingface_hub import InferenceClient
        client = InferenceClient(token=HUGGINGFACE_API_KEY)
        payload = {"inputs": inputs}
        if extra:
            payload.update(extra)
        out = client.post(json=payload, task=task, model=model_id)
        # Normalizar salida (puede venir como str, dict o lista)
        if isinstance(out, str):
            return out
        if isinstance(out, list) and out and isinstance(out[0], dict):
            # t√≠picamente [{'summary_text': '...'}] o [{'translation_text': '...'}]
            return out[0].get("summary_text") or out[0].get("translation_text") or str(out[0])
        if isinstance(out, dict):
            return out.get("generated_text") or out.get("summary_text") or out.get("translation_text") or str(out)
        return str(out)
    except Exception as e:
        return f"‚ö†Ô∏è Error calling Hugging Face Inference API: {e}"

def hf_summarization(text: str, model_id="facebook/bart-large-cnn"):
    # Evito kwargs incompatibles entre versiones (max_length/max_new_tokens).
    return hf_post(task="summarization", inputs=text, model_id=model_id)

def hf_translate_es_en(text: str, model_id="Helsinki-NLP/opus-mt-es-en"):
    # Traducci√≥n dedicada (mucho m√°s estable que pedirle a un LLM generativo).
    return hf_post(task="translation", inputs=text, model_id=model_id)

def hf_text_generation(prompt: str, model_id="Qwen/Qwen2.5-0.5B-Instruct", temperature=0.3, max_tokens=256):
    # Intento con text-generation. Si el modelo no soporta ese task, hago fallback a un T5 (text2text).
    out = hf_post(
        task="text-generation",
        inputs=prompt,
        model_id=model_id,
        extra={"parameters": {"max_new_tokens": int(max_tokens), "temperature": float(temperature)}},
    )
    # Si el backend se queja de "conversational" u otro task, pruebo text2text con FLAN-T5.
    if "not supported for task" in str(out).lower():
        out = hf_post(
            task="text2text-generation",
            inputs=prompt,
            model_id="google/flan-t5-base",
            extra={"parameters": {"max_new_tokens": int(max_tokens)}},
        )
    return out

# ====== Streamlit UI ======
st.set_page_config(page_title="Taller IA: OCR + LLM", page_icon="üß†", layout="wide")
st.title("üß† Taller IA: OCR + LLM (Streamlit)")
st.caption("App demo: OCR de im√°genes + an√°lisis con LLMs (GROQ / Hugging Face)")

with st.sidebar:
    st.header("‚öôÔ∏è Configuraci√≥n")
    provider = st.radio("Proveedor NLP", ["GROQ", "Hugging Face"], index=0)
    temperature = st.slider("Temperature (creatividad)", 0.0, 1.5, 0.3, 0.1)
    max_tokens = st.slider("max_tokens (longitud de respuesta)", 32, 2048, 512, 32)

    if provider == "GROQ":
        groq_model = st.selectbox(
            "Modelo GROQ",
            ["llama-3.1-8b-instant", "llama3-8b-8192", "mixtral-8x7b-32768"],
            index=0
        )
    else:
        hf_model = st.selectbox(
            "Modelo Hugging Face (task depende abajo)",
            [
                "facebook/bart-large-cnn",        # summarization
                "Qwen/Qwen2.5-0.5B-Instruct",     # text-generation
                "Helsinki-NLP/opus-mt-es-en",     # translation
            ],
            index=0
        )

# Persist state for extracted text
if "extracted_text" not in st.session_state:
    st.session_state["extracted_text"] = ""

st.subheader("üì∑ M√≥dulo 1: Lector de Im√°genes (OCR)")

uploaded = st.file_uploader("Sube una imagen (.png, .jpg, .jpeg)", type=["png", "jpg", "jpeg"])

col1, col2 = st.columns([1,1])
with col1:
    if uploaded is not None:
        image = Image.open(uploaded)
        st.image(image, caption="Imagen cargada", use_column_width=True)
        if st.button("üîé Extraer texto (OCR)"):
            with st.spinner("Ejecutando OCR..."):
                st.session_state["extracted_text"] = do_ocr(image)
            st.success("¬°Texto extra√≠do! Revisa el panel derecho.")
with col2:
    st.text_area("üìù Texto extra√≠do (editable)", key="extracted_text", height=300)

st.divider()
st.subheader("üß© M√≥dulo 2: Cerebro Ling√º√≠stico (LLM)")

task = st.selectbox(
    "Selecciona la tarea sobre el texto",
    ["Resumir en 3 puntos clave", "Identificar entidades principales", "Traducir al ingl√©s", "Explicaci√≥n general"]
)

def build_prompt(task_name: str, text: str):
    if task_name == "Resumir en 3 puntos clave":
        return f"Resume el siguiente texto en exactamente 3 vi√±etas claras y concisas:\n\nTexto:\n{text}"
    elif task_name == "Identificar entidades principales":
        return (
            "Extrae ENTIDADES (Personas, Organizaciones, Lugares, Fechas) y mu√©stralas en lista con categor√≠a y valor.\n"
            f"Texto:\n{text}"
        )
    elif task_name == "Traducir al ingl√©s":
        return f"Translate the following Spanish text into clear, natural English:\n\n{text}"
    else:
        return f"Deliver a clear, structured explanation of the following text focusing on the key ideas:\n\n{text}"

# Auto-selecci√≥n de modelo HF v√°lido por tarea
def pick_hf_model(task_name: str, chosen: str) -> str:
    if task_name == "Resumir en 3 puntos clave":
        return "facebook/bart-large-cnn"
    if task_name == "Traducir al ingl√©s":
        return "Helsinki-NLP/opus-mt-es-en"
    # Entidades / Explicaci√≥n -> generativo
    # Si el usuario dej√≥ BART (incompatible), forzamos Qwen
    if "bart" in (chosen or "").lower():
        return "Qwen/Qwen2.5-0.5B-Instruct"
    return chosen or "Qwen/Qwen2.5-0.5B-Instruct"

analyze = st.button("ü§ñ Analizar Texto")

if analyze:
    if not st.session_state["extracted_text"]:
        st.warning("Primero extrae o pega texto en el recuadro de la derecha.")
    else:
        text = st.session_state["extracted_text"]
        with st.spinner(f"Analizando con {provider}..."):
            if provider == "GROQ":
                if not GROQ_API_KEY:
                    st.error("Falta GROQ_API_KEY en tu .env")
                else:
                    prompt = build_prompt(task, text)
                    out = groq_chat_completion(prompt, model_name=groq_model, temperature=temperature, max_tokens=max_tokens)
                    st.markdown("**Salida (GROQ):**")
                    st.markdown(out)
            else:
                if not HUGGINGFACE_API_KEY:
                    st.error("Falta HUGGINGFACE_API_KEY en tu .env")
                else:
                    hf_used = pick_hf_model(task, hf_model)

                    if task == "Resumir en 3 puntos clave":
                        out = hf_summarization(text, model_id=hf_used)
                    elif task == "Traducir al ingl√©s":
                        out = hf_translate_es_en(text, model_id=hf_used)
                    else:
                        prompt = build_prompt(task, text)
                        out = hf_text_generation(
                            prompt,
                            model_id=hf_used,
                            temperature=temperature,
                            max_tokens=max_tokens
                        )

                    st.markdown("**Salida (Hugging Face):**")
                    st.markdown(out)

st.divider()
st.subheader("üß™ M√≥dulo 3: Flexibilidad y Experimentaci√≥n")
st.markdown(
    "- Cambia **Proveedor NLP** en la barra lateral (GROQ vs Hugging Face)\n"
    "- Ajusta **temperature** y **max_tokens** y observa c√≥mo cambia la salida\n"
    "- Prueba con im√°genes de distinta calidad para ver el impacto del OCR"
)

st.info("Consejo: si el modelo OCR tarda en cargar, es normal la primera vez. Gracias al cach√©, las siguientes ejecuciones ser√°n m√°s r√°pidas.")
